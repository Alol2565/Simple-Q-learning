# -*- coding: utf-8 -*-
"""RL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LFsDuEAerajH1vnULc6ZlbHsuN5MlydL
"""

import random
import pandas
from pprint import pprint
import numpy as np
import pandas as pd
from math import sqrt
from google.colab import drive
import matplotlib.pyplot as plt
import imageio

drive.mount('/content/drive')
path = '/content/drive/My Drive/IS_HW5/ENV.map'
output_path = '/content/drive/My Drive/IS_HW5/'
map = pd.read_csv(path,header = None)
map = map.to_numpy()
n = 15
actions = np.array(['up', 'down', 'left', 'right'])
trajectory = np.zeros((n,n))
p = 0.05
R2 = 30
# costs = c1,starting point, c2,R1,R2,R3,gone
costs = [-0.01, 'starting point', -20, 100, R2, -100, -0.5]

# used to select action to take
def policy(q,actions, state, t = 1):
    p = np.array([q[x,state[0],state[1]]/t for x in range(actions.size)])
    prob_actions = np.exp(p) / np.sum(np.exp(p))
    cumulative_probability = 0.0
    choice = random.uniform(0,1)
    for a,pr in enumerate(prob_actions):
        cumulative_probability += pr
        if cumulative_probability > choice:
            return a
    return 1

def update(q,actions, state, action, reward, nextstate, alpha = 0.01, gamma = 0.9):
    # select action that yields greatest stored value for nextstate and store its value
    qa = max([q[a,nextstate[0],nextstate[1]] for a in range(actions.size)])
    q[action,state[0],state[1]] += alpha * (reward + gamma * qa - q[action,state[0],state[1]])

def env(state, action, map , costs, trajectory, pe = 0.05, n = 15):
    done = False
    choise = np.random.uniform(0, 1)
    error_action = [0, 1, 2, 3]
    error_action.remove(action)
    if(choise < pe):
        action = random.choice(error_action)
    newstate = state
    if(action == 0 and state[0] > 0):
        newstate = np.array([state[0] - 1,state[1]])
    if(action == 1 and state[0] < n - 1):
        newstate = np.array([state[0] + 1,state[1]])
    if(action == 2 and state[1] > 0):
        newstate = np.array([state[0], state[1] - 1])
    if(action == 3 and state[1] < n - 1):
        newstate = np.array([state[0], state[1] + 1])
    cost_type = map[newstate[0], newstate[1]]
    reward = 0
    if(trajectory[newstate[0], newstate[1]] == 0):
        trajectory[newstate[0], newstate[1]] = 1
    else:
        trajectory[newstate[0], newstate[1]] += 1
        if(cost_type == 4):
            reward = 0
        reward += trajectory[newstate[0], newstate[1]] * costs[6]
    if(cost_type == 3):
        done = True
    if(cost_type == 1):
        reward += costs[0]
    else:
        reward += costs[cost_type]
    return [newstate, reward, done]

def learn(epochs, actions,costs, pe ,map, alpha, gamma, temp):
    state =  np.array([0, 0])
    n = 15
    q = np.zeros((actions.size,n,n))
    for i in range(epochs):
        trajectory = np.zeros((n,n))
        state[:] = np.floor(np.random.rand(1,2) * n)
        while True:
            action = policy(q,actions, state, temp)
            [newstate, reward, done] = env(state, action, map , costs, trajectory, pe, n)
            update(q,actions, state, action, reward, newstate, alpha, gamma)
            state = newstate
            if done:
                break
            if temp > 1.0:
                temp -= 0.01
    return q
def play(q,actions,costs, pe ,map,temp = 1):
    movments = []
    total_reward = 0
    R = []
    state =  np.array([0,0])
    trajectory = np.zeros((n,n))
    while True:
        movments.append(state)
        action = policy(q,actions,state, temp)
        [newstate, reward, done] = env(state, action, map , costs, trajectory, 0.05, 15)
        total_reward += reward
        R.append(total_reward)
        state = newstate
        if done:
            print('Done')
            break
        if temp > 1.0:
            temp -= 0.01
    print(total_reward)
    df = pandas.DataFrame(trajectory)
    print(df)
    plt.plot(R)
    plt.show()
    return movments

pe = 0.05
alpha = 0.1
gamma = 0.9
temp = 1
R2 = 0
R3 = 100
epochs = 3000
# costs = c1,starting point, c2,R1,R2,R3,gone
costs = [-0.01, 'starting point', -20, R3, R2, -100, -0.5]
q = learn(epochs, actions,costs, pe ,map, alpha, gamma, temp)
print(costs)

movments = play(q,actions,costs, pe ,map, 1)

im = []
i = 0
for move in movments:
    plt.scatter(move[1],n - move[0],c='Red')
    plt.xlim(-1, n + 1)
    plt.ylim(-1, n + 1)
    filepath = output_path + 'R2=0/' + str(i) +'_move.png'
    plt.savefig(filepath)
    im.append(filepath)
    i += 1
images = [imageio.imread(filepath) for filepath in im]
imageio.mimwrite(output_path + 'R2=0/' + 'movie.gif', images, fps=1000)

pe = 0.05
alpha = 0.1
gamma = 0.9
temp = 3
R2 = 300
R3 = 100
epochs = 10000
# costs = c1,starting point, c2,R1,R2,R3,gone
costs = [-0.01, 'starting point', -20, R3, R2, -100, -5]
q = learn(epochs, actions,costs, pe ,map, alpha, gamma, temp)
print(costs)

movments = play(q,actions,costs, pe ,map, 1)

im = []
i = 0
for move in movments:
    plt.scatter(move[1],n - move[0],c='Blue')
    plt.xlim(-1, n + 1)
    plt.ylim(-1, n + 1)
    filepath = output_path + 'R2=300/' + str(i) +'_move30.png'
    plt.savefig(filepath)
    im.append(filepath)
    i += 1

import imageio
images = [imageio.imread(filepath) for filepath in im]
imageio.mimwrite(output_path + 'R2=300/' + 'movie.gif', images, fps=1000)
